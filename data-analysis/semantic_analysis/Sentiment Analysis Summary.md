## Summary

Our analysis demonstrates that sentiment and linguistic expression in AI prompting are systematically influenced by user gender. Although the dataset is overall positively skewed, with a majority of prompts categorized as positive and an average VADER compound of 0.329, clear gender-based differences emerge. Female users consistently produced prompts with significantly higher compound sentiment scores (M=0.544) than male users (M=0.305), a difference that was robust under independent-samples t-testing (*p* < 0.001). Importantly, this effect is replicated across both VADER and TextBlob sentiment engines, minimizing the risk of model-specific bias. Beyond sentiment scores, linguistic analysis reveals that female users generated longer prompts (41.6 vs. 25.1 words on average) with more sentences of shorter mean length, while male users favored fewer, longer sentences. This suggests divergent prompting strategies: women employ a more conversational, incremental construction style, whereas men adopt a more compact, directive style. Complementing these structural differences, women’s prompts contained a richer emotional lexicon (more positive and negative emotion words) and greater first-person pronoun usage, indicating a higher degree of personalization and interpersonal framing.

Taken together, these findings provide quantitative evidence that gendered communication styles manifest in human–AI interaction, extending prior sociolinguistic theories into the domain of prompt engineering. Higher positivity and richer affective vocabulary among female participants indicate an interpersonally expressive orientation, aligning with hypotheses (H1a, H1c) that women exhibit greater socio-emotional engagement in language use. Conversely, the brevity, reduced affective vocabulary, and longer sentence construction among male participants reflect a direct, transactional prompting style, consistent with H1b and H0c. Crucially, these stylistic divergences are not merely superficial: sentiment and linguistic packaging shape how AI systems parse and respond to requests. Thus, the observed gender effects raise important implications for fairness and inclusivity in AI design. If language models are more attuned to certain linguistic styles, then systematic differences in positivity, emotional expression, or personalization could reinforce disparities in perceived usefulness and satisfaction across user groups. These findings underscore the need for AI evaluation frameworks that explicitly account for **gender-related linguistic variation in prompting behavior**, ensuring equitable performance across diverse communication styles.
